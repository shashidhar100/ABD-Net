{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a91556c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1618560087930,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "4a91556c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smU7S3Eus33t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1618560290317,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "smU7S3Eus33t",
    "outputId": "28b41cfa-b0d0-49ff-be6e-76d71cb0b5ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Decomposer\n"
     ]
    }
   ],
   "source": [
    "cd \"/content/drive/MyDrive/Decomposer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "p4Rdu95is-TK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30037,
     "status": "ok",
     "timestamp": 1618560201374,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "p4Rdu95is-TK",
    "outputId": "718a9fe0-8b8a-4a1f-bfbe-5c081fd45993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ddebc43",
   "metadata": {
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1618560919960,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "9ddebc43"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91181248",
   "metadata": {
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1618560297831,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "91181248"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    \"\"\"\n",
    "    q : query matrix (..,len_q,depth)\n",
    "    k : key matrix (..,len_k,depth)\n",
    "    v : value matrix (..,len_v,depth_v) len_v == len_k\n",
    "    mask : mask to be used before softmax (..,len_q,len_k)\n",
    "    \"\"\"\n",
    "    \n",
    "    qk = torch.matmul(q,torch.transpose(k,-2,-1)) # (..,len_q,len_k)\n",
    "    \n",
    "    # scaling the qk\n",
    "    dk = torch.tensor(k.shape[-1]).float()\n",
    "    scaled_qk = qk / torch.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_qk += (mask * -1e9)\n",
    "        \n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    \n",
    "    attention_weights = torch.nn.functional.softmax(scaled_qk,dim = -1) #(..,len_q,len_k)\n",
    "    \n",
    "    output = torch.matmul(attention_weights,v) #(len_q,depth_v)\n",
    "    \n",
    "    return output,attention_weights\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d5e464",
   "metadata": {
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1618560356767,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "84d5e464"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,input_dims):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model #output feature dimension\n",
    "        self.input_dims = input_dims\n",
    "        \n",
    "        #d_model is the depth of each head hence it should be divisible by number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = torch.nn.Linear(input_dims,d_model)\n",
    "        print(\"weights shape:\", self.wq.weight.data.shape)\n",
    "        \n",
    "        self.wk = torch.nn.Linear(input_dims,d_model)\n",
    "        self.wv = torch.nn.Linear(input_dims,d_model)\n",
    "        \n",
    "        self.projector = torch.nn.Linear(d_model,d_model)\n",
    "        \n",
    "    \n",
    "    def split_heads(self,x,batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len(N), depth)\n",
    "        \"\"\"\n",
    "        x = torch.reshape(x,(batch_size,-1,self.num_heads,self.depth))\n",
    "        return torch.transpose(x,2,1)\n",
    "    \n",
    "    def forward(self,v,k,q,mask):\n",
    "        batch_size = q.size()[0]\n",
    "        \n",
    "        q = self.wq(q) #(batch_size,seq_len(N),d_model)\n",
    "        k = self.wk(k) #(batch_size,seq_len(N),d_model)\n",
    "        v = self.wv(v) #(batch_size,seq_len(N),d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, len_q, len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = torch.transpose(scaled_attention,2,1)  # (batch_size, len_q, num_heads, depth)\n",
    "        \n",
    "        concat_attention = torch.reshape(scaled_attention,(batch_size, -1, self.d_model))   # (batch_size,len_q, d_model)\n",
    "\n",
    "        \n",
    "        output = self.projector(concat_attention)  # (batch_size,len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567afeab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 919,
     "status": "ok",
     "timestamp": 1618560727882,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "567afeab",
    "outputId": "561f4a6e-b771-43b6-ddfd-36c5bdd5ff26"
   },
   "outputs": [],
   "source": [
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8,input_dims=7)\n",
    "# y = torch.rand((1, 1024, 7))\n",
    "# out,att = temp_mha(y,y,y,mask=None)\n",
    "# out.shape,att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ee04d3",
   "metadata": {
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1618560359240,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "f0ee04d3"
   },
   "outputs": [],
   "source": [
    "class point_wise_feed_forward_network(nn.Module):\n",
    "    def __init__(self,d_model,dff,input_dims):\n",
    "        super(point_wise_feed_forward_network, self).__init__()\n",
    "        self.d_model = d_model # output feature dimension\n",
    "        self.dff = dff # intermediate feature dimension\n",
    "        self.input_dims = input_dims \n",
    "        \n",
    "        self.dense1 = torch.nn.Linear(self.input_dims,self.dff) # (batch_size, seq_len(N), dff)\n",
    "        self.dense2 = torch.nn.Linear(self.dff,self.d_model)  # (batch_size, seq_len(N), d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.dense1(x))\n",
    "        output = self.dense2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba370fc1",
   "metadata": {
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1618560360815,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "ba370fc1"
   },
   "outputs": [],
   "source": [
    "# sample_ffn = point_wise_feed_forward_network(512, 2048,512)\n",
    "# sample_ffn(torch.rand((64, 50, 512))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "092cfce8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1182,
     "status": "ok",
     "timestamp": 1618560363083,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "092cfce8"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,d_model,dff,num_heads,input_dims,if_ffn=False,if_layer_norm=False,\n",
    "                if_dropout = False,dropout_p = 0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dims = input_dims\n",
    "        self.if_ffn = if_ffn\n",
    "        self.if_layer_norm = if_layer_norm\n",
    "        self.if_dropout = if_dropout\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model=self.d_model, num_heads=self.num_heads,input_dims = self.input_dims)\n",
    "        if self.if_ffn:\n",
    "            self.ffn = point_wise_feed_forward_network(d_model = self.d_model,dff=self.dff,input_dims = self.input_dims)\n",
    "        if self.if_layer_norm:\n",
    "            self.layernorm1 = torch.nn.LayerNorm(eps = 1e-6)\n",
    "            self.layernorm2 = torch.nn.LayerNorm(eps = 1e-6)\n",
    "        if self.if_dropout:\n",
    "            self.dropout1 = torch.nn.Dropout(self.dropout_p)\n",
    "            self.dropout2 = torch.nn.Dropout(self.dropout_p)\n",
    "        \n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        out, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        if self.if_dropout:\n",
    "            out = self.dropout1(out)\n",
    "        if self.if_layer_norm:\n",
    "            out = self.layernorm1(x + out)\n",
    "            \n",
    "        \n",
    "        if self.if_ffn:\n",
    "            out = self.ffn(out)\n",
    "            if self.if_dropout:\n",
    "                out = self.dropout2(out)\n",
    "            if self.if_layer_norm:\n",
    "                out = self.layernorm2(x + out)\n",
    "        return out\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3d229f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1130,
     "status": "ok",
     "timestamp": 1618560366160,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "f3d229f8",
    "outputId": "294cb785-3853-461b-be67-f381c774d36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: torch.Size([512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 43, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = Encoder(d_model=512, num_heads=8, dff=2048,input_dims=512)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    torch.rand((64, 43, 512)), None)\n",
    "sample_encoder_layer_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d789509e",
   "metadata": {
    "executionInfo": {
     "elapsed": 1020,
     "status": "ok",
     "timestamp": 1618561591212,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "d789509e"
   },
   "outputs": [],
   "source": [
    "class Decomposer(nn.Module):\n",
    "    def __init__(self,d_model_list=[128,256,512],dff=1024,num_heads=4,input_dims=3,if_ffn=False,\n",
    "                 if_layer_norm=False,if_dropout = False,dropout_p = 0.5,mlp=[16,32,64],n_neighbors=32,\n",
    "                no_of_classes = 10,if_first_hd=True,first_hd_dim=64,if_group_norm=True,if_avg_pool=True):\n",
    "        super(Decomposer, self).__init__()\n",
    "        self.d_model_list = d_model_list\n",
    "        self.dff = dff\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dims = input_dims\n",
    "        self.if_ffn = if_ffn\n",
    "        self.if_layer_norm = if_layer_norm\n",
    "        self.if_dropout = if_dropout\n",
    "        self.dropout_p = dropout_p\n",
    "        self.mlp = mlp\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.no_of_classes  = no_of_classes\n",
    "        self.if_first_hd = if_first_hd\n",
    "        self.first_hd_dim = first_hd_dim\n",
    "        self.if_group_norm = if_group_norm\n",
    "        self.if_avg_pool = if_avg_pool\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        if self.if_first_hd:\n",
    "            input_dims = self.first_hd_dim + 3\n",
    "        else:\n",
    "            input_dims = self.input_dims \n",
    "        last_channel = input_dims \n",
    "        for out_channel in self.mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        \n",
    "        self.encoders = nn.ModuleList()\n",
    "        input_dims = self.mlp[-1]\n",
    "        for i in range(len(self.d_model_list)):\n",
    "            self.encoders.append(layers.Encoder(d_model = self.d_model_list[i],\n",
    "                                         dff = self.dff,\n",
    "                                         num_heads = self.num_heads,\n",
    "                                         input_dims = input_dims,\n",
    "                                         if_ffn = self.if_ffn,\n",
    "                                        if_layer_norm = self.if_layer_norm,\n",
    "                                        if_dropout = self.if_dropout,\n",
    "                                        dropout_p = self.dropout_p\n",
    "                                        ))\n",
    "            input_dims = self.d_model_list[i]\n",
    "        if self.if_first_hd:\n",
    "            self.first_hd_conv = torch.nn.Conv1d(self.input_dims,self.first_hd_dim,1)\n",
    "            self.first_hd_bn = nn.BatchNorm1d(self.first_hd_dim)\n",
    "        self.classifier_layer = torch.nn.Conv1d(self.d_model_list[-1],self.no_of_classes,1)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def forward(self,pc,mask):\n",
    "        pc = pc.permute(0, 2, 1) #(B,C+d,N) --> (B,N,C+d)\n",
    "        B,N,D = pc.size()\n",
    "        \n",
    "        xyz = pc[:,:,:3]\n",
    "        print(xyz.shape)\n",
    "        if D>3:\n",
    "            feat = pc[:,:,3:]\n",
    "        \n",
    "        pc = pc.permute(0, 2, 1) #(B,C+d,N) --> (B,N,C+d)\n",
    "        if self.if_first_hd:\n",
    "            pc = self.first_hd_conv(pc)\n",
    "            pc = self.first_hd_bn(pc)\n",
    "            feat = pc.permute(0, 2, 1)\n",
    "        \n",
    "        \n",
    "        print(pc.shape)\n",
    "        print(xyz.shape)\n",
    "        knn_idx = layers.knn(xyz,self.n_neighbors)\n",
    "        grouped_xyz = layers.index_points(xyz,knn_idx)\n",
    "        grouped_feat = layers.index_points(feat,knn_idx)\n",
    "        \n",
    "        if self.if_group_norm:\n",
    "            grouped_xyz_norm = layers.group_norm(xyz,grouped_xyz)\n",
    "        else:\n",
    "            grouped_xyz_norm = grouped_xyz\n",
    "        print(grouped_xyz_norm.shape)\n",
    "        if D>3:\n",
    "            feat_xyz = torch.cat((grouped_xyz_norm,grouped_feat),dim=3)\n",
    "        else:\n",
    "            feat_xyz = grouped_xyz\n",
    "        \n",
    "        feat_xyz = feat_xyz.permute(0,3,2,1)\n",
    "        print(feat_xyz.shape)\n",
    "        for i,conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            feat_xyz = F.relu(bn(conv(feat_xyz)))\n",
    "        print(\"after pointnet\",feat_xyz.shape)\n",
    "        \n",
    "        if self.if_avg_pool:\n",
    "            feat_xyz = torch.mean(feat_xyz,dim=2)\n",
    "            \n",
    "        else:\n",
    "            feat_xyz = torch.max(feat_xyz)\n",
    "        print(feat_xyz.shape)   \n",
    "        feat_xyz = feat_xyz.permute(0,2,1)\n",
    "        print(feat_xyz.shape)\n",
    "        for i,encoder in enumerate(self.encoders):\n",
    "            feat_xyz = encoder(feat_xyz,mask)\n",
    "        feat_xyz = feat_xyz.permute(0,2,1)\n",
    "        output = F.log_softmax(self.classifier_layer(feat_xyz),dim=1)\n",
    "        \n",
    "        output = output.permute(0,2,1)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0244bc89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18921,
     "status": "ok",
     "timestamp": 1618561709643,
     "user": {
      "displayName": "CEVI Interns",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjtCVmXa9wxihLuP1QtjnWQOoL_lIVEj27rx0n2=s64",
      "userId": "09571738473941606360"
     },
     "user_tz": -330
    },
    "id": "0244bc89",
    "outputId": "df800b74-aefa-44b0-a1f2-b4bbb758ec2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024, 3])\n",
      "torch.Size([16, 64, 1024])\n",
      "torch.Size([16, 1024, 3])\n",
      "torch.Size([16, 1024, 32, 3])\n",
      "torch.Size([16, 67, 32, 1024])\n",
      "after pointnet torch.Size([16, 64, 32, 1024])\n",
      "torch.Size([16, 64, 1024])\n",
      "torch.Size([16, 1024, 64])\n",
      "torch.Size([16, 1025, 3])\n",
      "torch.Size([16, 64, 1025])\n",
      "torch.Size([16, 1025, 3])\n",
      "torch.Size([16, 1025, 32, 3])\n",
      "torch.Size([16, 67, 32, 1025])\n",
      "after pointnet torch.Size([16, 64, 32, 1025])\n",
      "torch.Size([16, 64, 1025])\n",
      "torch.Size([16, 1025, 64])\n",
      "torch.Size([16, 1026, 3])\n",
      "torch.Size([16, 64, 1026])\n",
      "torch.Size([16, 1026, 3])\n",
      "torch.Size([16, 1026, 32, 3])\n",
      "torch.Size([16, 67, 32, 1026])\n",
      "after pointnet torch.Size([16, 64, 32, 1026])\n",
      "torch.Size([16, 64, 1026])\n",
      "torch.Size([16, 1026, 64])\n",
      "torch.Size([16, 1027, 3])\n",
      "torch.Size([16, 64, 1027])\n",
      "torch.Size([16, 1027, 3])\n",
      "torch.Size([16, 1027, 32, 3])\n",
      "torch.Size([16, 67, 32, 1027])\n",
      "after pointnet torch.Size([16, 64, 32, 1027])\n",
      "torch.Size([16, 64, 1027])\n",
      "torch.Size([16, 1027, 64])\n",
      "torch.Size([16, 1028, 3])\n",
      "torch.Size([16, 64, 1028])\n",
      "torch.Size([16, 1028, 3])\n",
      "torch.Size([16, 1028, 32, 3])\n",
      "torch.Size([16, 67, 32, 1028])\n",
      "after pointnet torch.Size([16, 64, 32, 1028])\n",
      "torch.Size([16, 64, 1028])\n",
      "torch.Size([16, 1028, 64])\n"
     ]
    }
   ],
   "source": [
    "net = Decomposer(input_dims=7)\n",
    "for i in range(5):\n",
    "  pc = torch.randn((16,7,1024+i))\n",
    "# from dataloader import \n",
    "  out = net(pc,None)\n",
    "  out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b3a88",
   "metadata": {
    "id": "049b3a88"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
